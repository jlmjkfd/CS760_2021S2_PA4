{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_pnas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tot import TopicsOverTime\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def main():\n",
    "    datapath = '../../data/pnas/'\n",
    "    resultspath = '../results/pnas_tot/'\n",
    "    documents_path = datapath + 'alltitles'\n",
    "    timestamps_path = datapath + 'alltimes'\n",
    "    stopwords_path = datapath + 'allstopwords'\n",
    "    tot_topic_vectors_path = resultspath + 'pnas_tot_topic_vectors.csv'\n",
    "    tot_topic_mixtures_path = resultspath + 'pnas_tot_topic_mixtures.csv'\n",
    "    tot_topic_shapes_path = resultspath + 'pnas_tot_topic_shapes.csv'\n",
    "    tot_pickle_path = resultspath + 'pnas_tot.pickle'\n",
    "\n",
    "    tot = TopicsOverTime()\n",
    "    documents, timestamps, dictionary = tot.GetPnasCorpusAndDictionary(documents_path, timestamps_path, stopwords_path)\n",
    "    par = tot.InitializeParameters(documents, timestamps, dictionary)\n",
    "    theta, phi, psi = tot.TopicsOverTimeGibbsSampling(par)\n",
    "    np.savetxt(tot_topic_vectors_path, phi, delimiter=',')\n",
    "    np.savetxt(tot_topic_mixtures_path, theta, delimiter=',')\n",
    "    np.savetxt(tot_topic_shapes_path, psi, delimiter=',')\n",
    "    tot_pickle = open(tot_pickle_path, 'wb')\n",
    "    pickle.dump(par, tot_pickle)\n",
    "    tot_pickle.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import copy\n",
    "import fileinput\n",
    "import random\n",
    "import scipy.special\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pickle\n",
    "from math import log\n",
    "\n",
    "class TopicsOverTime:\n",
    "    \n",
    "    def GetPnasCorpusAndDictionary(self, documents_path, timestamps_path, stopwords_path):\n",
    "        \n",
    "        # initialise variables\n",
    "        documents = []\n",
    "        timestamps = []\n",
    "        dictionary = set()\n",
    "        stopwords = set()\n",
    "        \n",
    "        # read stopwords from stopwords file into a set of stopwords\n",
    "        for line in fileinput.input(stopwords_path):\n",
    "            stopwords.update(set(line.lower().strip().split()))\n",
    "            \n",
    "        # read words from documents file, tokenise each document into a list of words\n",
    "        # append each word list to var `documents` to form a list of word lists\n",
    "        # append each word list to var `dictionary` to form a uniuqe set of words\n",
    "        for doc in fileinput.input(documents_path):\n",
    "            words = [word for word in doc.lower().strip().split() if word not in stopwords]\n",
    "            documents.append(words)\n",
    "            dictionary.update(set(words))\n",
    "        \n",
    "        # recognise the earliest date as 0s, then count the dates after in seconds\n",
    "        for timestamp in fileinput.input(timestamps_path):\n",
    "            num_titles = int(timestamp.strip().split()[0])\n",
    "            timestamp = float(timestamp.strip().split()[1])\n",
    "            timestamps.extend([timestamp for title in range(num_titles)])\n",
    "            \n",
    "        # normalise each date, range from 0 to 1\n",
    "        first_timestamp = timestamps[0]\n",
    "        last_timestamp = timestamps[len(timestamps)-1]\n",
    "        timestamps = [1.0*(t-first_timestamp)/(last_timestamp-first_timestamp) for t in timestamps]\n",
    "        \n",
    "        # convert var `dictionary` from set to list\n",
    "        dictionary = list(dictionary)\n",
    "        \n",
    "        # if condition returns True, nothing happens\n",
    "        # else, AssertionError is raised\n",
    "        assert len(documents) == len(timestamps)\n",
    "        \n",
    "        # return output\n",
    "        return documents, timestamps, dictionary\n",
    "\n",
    "    def CalculateCounts(self, par):\n",
    "        for d in range(par['D']):              # for each document\n",
    "            for i in range(par['N'][d]):         # for each word in each document\n",
    "                topic_di = par['z'][d][i]          # topic in doc d at word i\n",
    "                word_di = par['w'][d][i]           # word ID in doc d at word i\n",
    "                par['m'][d][topic_di] += 1         # distribution for topic,if word i in document d is assigned to this topic, +1 for this topic\n",
    "                par['n'][topic_di][word_di] += 1   # distribution for word, if word i was assigned to this topic, +1 for specific word for this topic\n",
    "                par['n_sum'][topic_di] += 1\n",
    "\n",
    "    def InitializeParameters(self, documents, timestamps, dictionary):\n",
    "        par = {}                        # dictionary of all parameters\n",
    "        par['dataset'] = 'pnas'         # dataset name\n",
    "        par['max_iterations'] = 100     # max number of iterations in gibbs sampling\n",
    "        par['T'] = 10                   # number of topics\n",
    "        par['V'] = len(dictionary)      # number of unique words in dictionary\n",
    "        par['N'] = [len(doc) for doc in documents]              # length of each document in documents\n",
    "        par['alpha'] = [50.0/par['T'] for _ in range(par['T'])] # alpha = 50 / number of topics. len(par['alpha']) = num of topics\n",
    "        par['beta'] = [0.1 for _ in range(par['V'])]            # beta = 0.1. len(par['beta']) = num of unique words in `dictionary`.\n",
    "        par['beta_sum'] = sum(par['beta'])                      # this is for TopicsOverTimeGibbsSampling()\n",
    "        par['psi'] = [[1 for _ in range(2)] for _ in range(par['T'])] # parameter of Beta distribution, this step is for initialisation\n",
    "        par['betafunc_psi'] = [scipy.special.beta( par['psi'][t][0], par['psi'][t][1] ) for t in range(par['T'])] # Beta distribution of time specific to topic\n",
    "        par['word_id'] = {dictionary[i]: i for i in range(len(dictionary))} # assign id for each word in dictionary\n",
    "        par['word_token'] = dictionary # assign a set of unique words from `dictionary` to `word_token`\n",
    "        par['z'] = [[random.randrange(0,par['T']) for _ in range(par['N'][d])] for d in range(par['D'])] # initialise - assign a random topic to each word in each document\n",
    "        par['t'] = [[timestamps[d] for _ in range(par['N'][d])] for d in range(par['D'])] # initialise - assign a random timestamp to each word in each document\n",
    "        par['w'] = [[par['word_id'][documents[d][i]] for i in range(par['N'][d])] for d in range(par['D'])] # assign word id to each word in each document\n",
    "        par['m'] = [[0 for t in range(par['T'])] for d in range(par['D'])] # initialise theta: proportion of topics in each document)\n",
    "        par['n'] = [[0 for v in range(par['V'])] for t in range(par['T'])] # initialise phi: word distribution for each topic\n",
    "        par['n_sum'] = [0 for t in range(par['T'])] # ?\n",
    "        np.set_printoptions(threshold=np.inf) # when the set has many elements, ensure that `...` is not printed\n",
    "        np.seterr(divide='ignore', invalid='ignore') # ignore zero division error\n",
    "        self.CalculateCounts(par)\n",
    "        return par\n",
    "\n",
    "    def GetTopicTimestamps(self, par):\n",
    "        \n",
    "        topic_timestamps = []\n",
    "        \n",
    "        # for each topic in all topics\n",
    "        for topic in range(par['T']):\n",
    "            \n",
    "            current_topic_timestamps = []\n",
    "            \n",
    "            # if topic of word i in current doc d == current topic, then timestamp for word i in doc d remains the same\n",
    "            # if not, then timestamp for word i in doc d is updated to 0\n",
    "            current_topic_doc_timestamps = [[ (par['z'][d][i]==topic)*par['t'][d][i] for i in range(par['N'][d])] for d in range(par['D'])]\n",
    "            \n",
    "            # for each document in all documents\n",
    "            for d in range(par['D']):\n",
    "                # keep only timestamps that do not equal to 0. Use list(current_topic_doc_timestamps[d]) to see remaining values\n",
    "                current_topic_doc_timestamps[d] = filter(lambda x: x!=0, current_topic_doc_timestamps[d])\n",
    "            \n",
    "            # create a list of non-zero timestamps for each topic\n",
    "            for timestamps in current_topic_doc_timestamps:\n",
    "                current_topic_timestamps.extend(timestamps)\n",
    "            \n",
    "            # if condition returns True, nothing happens\n",
    "            # else, AssertionError is raised\n",
    "            assert current_topic_timestamps != []\n",
    "            \n",
    "            # create a list of timestamps for all topics\n",
    "            topic_timestamps.append(current_topic_timestamps)\n",
    "            \n",
    "        return topic_timestamps\n",
    "        # topic timestamps exclude timestamps of the first few data rows (given their timestamps = 0)\n",
    "\n",
    "    def GetMethodOfMomentsEstimatesForPsi(self, par):\n",
    "        \"\"\"\n",
    "        estimate value of psi\n",
    "        psi is distribution of words\n",
    "        :param par: a dictionary for all parameters\n",
    "        :return: value of psi\n",
    "        \"\"\"\n",
    "        # get topic timestamps\n",
    "        topic_timestamps = self.GetTopicTimestamps(par)\n",
    "        # make a list of topic timestamps, each element is [1,1]\n",
    "        psi = [[1 for _ in range(2)] for _ in range(len(topic_timestamps))]\n",
    "        # for length of topic timestamps list\n",
    "        for i in range(len(topic_timestamps)):\n",
    "            # get current topic timestamp\n",
    "            current_topic_timestamps = topic_timestamps[i]\n",
    "            # get current topic timestamp mean\n",
    "            timestamp_mean = np.mean(current_topic_timestamps)\n",
    "            # get current topic timestamp variance\n",
    "            timestamp_var = np.var(current_topic_timestamps)\n",
    "            # set varaince to be non-zero\n",
    "            if timestamp_var == 0:\n",
    "                timestamp_var = 1e-6\n",
    "            # beta distribution comes from formula from orginal paper\n",
    "            common_factor = timestamp_mean*(1-timestamp_mean)/timestamp_var - 1\n",
    "            # + 1 to make sure Beta parameters larger than 0 (Beta distribution needs parameters > 0)\n",
    "            # Beta(a,b), a is psi[i][0], b is psi[i][1]\n",
    "            psi[i][0] = 1 + timestamp_mean*common_factor\n",
    "            psi[i][1] = 1 + (1-timestamp_mean)*common_factor\n",
    "        return psi\n",
    "\n",
    "    def ComputePosteriorEstimatesOfThetaAndPhi(self, par):\n",
    "        \"\"\"\n",
    "        estimate value of theta and phi\n",
    "            theta is porporiton of topics in each document\n",
    "            phi is word distribution for each topic\n",
    "        :param par: a dictionary for all parameters\n",
    "        :return: value of psi\n",
    "        \"\"\"\n",
    "        \n",
    "        theta = copy.deepcopy(par['m'])\n",
    "        phi = copy.deepcopy(par['n'])\n",
    "        \n",
    "        # for each document in document\n",
    "        for d in range(par['D']):\n",
    "            # for a document without any topic\n",
    "            # make its distribution to be [1/num_topics for i in range(num_topics)]\n",
    "            if sum(theta[d]) == 0:\n",
    "                theta[d] = np.asarray([1.0/len(theta[d]) for _ in range(len(theta[d]))])\n",
    "            # normalise each topic proportion, so the sum of all topic proportions is 1\n",
    "            else:\n",
    "                theta[d] = np.asarray(theta[d])\n",
    "                theta[d] = 1.0*theta[d]/sum(theta[d])\n",
    "        theta = np.asarray(theta)\n",
    "        \n",
    "        # for each topic in topics\n",
    "        for t in range(par['T']):\n",
    "            # for a topic without any words\n",
    "            # make its distribution to be [1/num_words for i in range(num_words)]\n",
    "            if sum(phi[t]) == 0:\n",
    "                phi[t] = np.asarray([1.0/len(phi[t]) for _ in range(len(phi[t]))])\n",
    "            # normalise each word's probability under a topic, so the sum of word probabilities equal to 1\n",
    "            else:\n",
    "                phi[t] = np.asarray(phi[t])\n",
    "                phi[t] = 1.0*phi[t]/sum(phi[t])\n",
    "        phi = np.asarray(phi)\n",
    "\n",
    "        return theta, phi\n",
    "\n",
    "    def TopicsOverTimeGibbsSampling(self, par):\n",
    "        \"\"\"\n",
    "        TOT uses Gibbs sampling \n",
    "        :param par: a dictionary for all parameters\n",
    "        :return: value of psi\n",
    "        \"\"\"\n",
    "        for iteration in range(par['max_iterations']):\n",
    "            # for each document in documents\n",
    "            for d in range(par['D']):\n",
    "                for i in range(par['N'][d]):\n",
    "                    word_di = par['w'][d][i]\n",
    "                    t_di = par['t'][d][i]\n",
    "\n",
    "                    # initialise `m`, `n`, `n_sum`\n",
    "                    old_topic = par['z'][d][i]\n",
    "                    par['m'][d][old_topic] -= 1\n",
    "                    par['n'][old_topic][word_di] -= 1\n",
    "                    par['n_sum'][old_topic] -= 1\n",
    "\n",
    "                    # formula from original paper, without the -1 operation\n",
    "                    # removing the -1 operation should not impact the results too much\n",
    "                    topic_probabilities = []\n",
    "                    for topic_di in range(par['T']):\n",
    "                        psi_di = par['psi'][topic_di] # psi is parameter of Beta distribution of time specific to topic\n",
    "                        topic_probability = 1.0 * (par['m'][d][topic_di] + par['alpha'][topic_di]) #\n",
    "                        topic_probability *= ((1-t_di)**(psi_di[0]-1)) * ((t_di)**(psi_di[1]-1))\n",
    "                        topic_probability /= par['betafunc_psi'][topic_di]\n",
    "                        topic_probability *= (par['n'][topic_di][word_di] + par['beta'][word_di])\n",
    "                        topic_probability /= (par['n_sum'][topic_di] + par['beta_sum'])\n",
    "                        topic_probabilities.append(topic_probability)\n",
    "                    sum_topic_probabilities = sum(topic_probabilities)\n",
    "                    if sum_topic_probabilities == 0:\n",
    "                        topic_probabilities = [1.0/par['T'] for _ in range(par['T'])]\n",
    "                    else:\n",
    "                        topic_probabilities = [p/sum_topic_probabilities for p in topic_probabilities]\n",
    "                    \n",
    "                    # for each word, according to above probabilities of topic, randomly assign a new topic\n",
    "                    new_topic = list(np.random.multinomial(1, topic_probabilities, size=1)[0]).index(1)\n",
    "                    par['z'][d][i] = new_topic\n",
    "                    par['m'][d][new_topic] += 1\n",
    "                    par['n'][new_topic][word_di] += 1\n",
    "                    par['n_sum'][new_topic] += 1\n",
    "                \n",
    "                # print an iteration status message for every 1000 iterations\n",
    "                if d%1000 == 0:\n",
    "                    print('Done with iteration {iteration} and document {document}'.format(iteration=iteration, document=d))\n",
    "            \n",
    "            par['psi'] = self.GetMethodOfMomentsEstimatesForPsi(par)\n",
    "            par['betafunc_psi'] = [scipy.special.beta( par['psi'][t][0], par['psi'][t][1] ) for t in range(par['T'])]\n",
    "        \n",
    "        par['m'], par['n'] = self.ComputePosteriorEstimatesOfThetaAndPhi(par)\n",
    "        \n",
    "        return par['m'], par['n'], par['psi']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
